{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import scipy.sparse\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn import utils\n",
    "import multiprocessing\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = np.load(\"preprocessed_texts.npy\", allow_pickle = True)\n",
    "labels = np.load(\"preprocessed_labels.npy\", allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration for Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF | CountVectorization | BERT | DOC2VEC\n",
    "method = \"TF-IDF\"\n",
    "dimension = 200 # Doc2Vec vector dimension\n",
    "epochs = 10 # Doc2Vec training epochs\n",
    "min_df = 0.001 # TF-IDF min df\n",
    "max_df = 0.9 # TF-IDF max df\n",
    "use_idf = False \n",
    "\n",
    "with open('config.json', 'r') as openfile:\n",
    "    json_object = json.load(openfile)\n",
    "    json_object[\"transformation_method\"] = method\n",
    "    json_object[\"doc2vec_dimension\"] = dimension\n",
    "    json_object[\"doc2vec_epochs\"] = epochs\n",
    "    json_object[\"min_df\"] = min_df\n",
    "    json_object[\"max_df\"] = max_df\n",
    "    json_object[\"use_idf\"] = use_idf\n",
    "with open(\"config.json\", \"w\") as outfile:\n",
    "    outfile.write(json.dumps(json_object))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorization\n",
    "We cannot use this vectorization methods because the fake news differ in the length of texts from the true news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == \"CountVectorization\":\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\")\n",
    "    all_tokens = vectorizer.fit_transform(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13691 features after using vectorizer.\n"
     ]
    }
   ],
   "source": [
    "if method == \"TF-IDF\":\n",
    "    vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, use_idf=use_idf) #critical values selected from research papers \n",
    "    all_tokens = vectorizer.fit_transform(texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(f\"{len(feature_names)} features after using vectorizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == \"BERT\":\n",
    "    tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\"\n",
    "    tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "\n",
    "    print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "    print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n",
    "\n",
    "    \n",
    "    bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "    #if \"OSError: SavedModel file does not exist\" occurs, navigate to the indicated folder and delete it\n",
    "    \n",
    "    text_test = texts\n",
    "    text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "    #print(text_test)\n",
    "    print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "    print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "    print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"]}')\n",
    "    print(f'Input Mask : {text_preprocessed[\"input_mask\"]}')\n",
    "    print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DOC2Vec\n",
    "Converting the document into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdf\n"
     ]
    }
   ],
   "source": [
    "if method == \"DOC2VEC\":\n",
    "    tagged_texts = []\n",
    "    for i, text in enumerate(texts):\n",
    "            word_tokens = word_tokenize(text)\n",
    "            tagged_texts.append(TaggedDocument(word_tokens, [i]))\n",
    "    cores = multiprocessing.cpu_count()\n",
    "\n",
    "    model = Doc2Vec(dm=0, vector_size=dimension, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model.build_vocab([x for x in tqdm(tagged_texts)])\n",
    "    for epoch in range(epochs):\n",
    "        model.train(utils.shuffle([x for x in tqdm(tagged_texts)]), total_examples=len(tagged_texts), epochs=1)\n",
    "        model.alpha -= 0.002\n",
    "        model.min_alpha = model.alpha\n",
    "    vector_list = []\n",
    "    for i in range(len(texts)):\n",
    "        vector_list.append(model.dv[i])\n",
    "    all_tokens = scipy.sparse.csr_matrix(vector_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == \"TF-IDF\" or method == \"DOC2VEC\":\n",
    "    scipy.sparse.save_npz(\"training_input\", all_tokens)\n",
    "    np.save(\"training_labels\", labels)\n",
    "if method == \"TF-IDF\":\n",
    "    np.save(\"feature_names\", feature_names)\n",
    "\n",
    "if method == \"BERT\":\n",
    "    a_file = open(\"bert_preprocessed.pkl\", \"wb\")\n",
    "    pickle.dump(text_preprocessed, a_file)\n",
    "    a_file.close()\n",
    "    np.save(\"training_labels\", labels)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3008482b51f5a95e2ad66327b5a1148e0bb88cb461a0c6926c59de3843ba3656"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
