{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Kaggle Data\n",
    "\n",
    "## 1. Imports \n",
    "\n",
    "Steps:\n",
    "1. Import \"pandas\" to import a .csv file from the file system.\n",
    "2. Import \"TfidfVectorizer\" to convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "3. Import \"CountVectorizer\" to convert a collection of text documents to a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Critical Imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read file\n",
    "\n",
    "Steps: \n",
    "1. Importing datasets ( https://www.kaggle.com/code/vpkprasanna/basic-text-cleaning-wordcloud-and-n-gram-analysis#Merging-true-and-fake-news-dataset )\n",
    "2. Converting datasets\n",
    "3. Combining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# importing the fake and the true dataset from the file system\n",
    "fake = pd.read_csv(\"Fake.csv\")\n",
    "true = pd.read_csv(\"True.csv\")\n",
    "\n",
    "# Convert each text of a dataset to a NumPy Array\n",
    "fake_texts = fake[\"text\"].to_numpy()\n",
    "true_texts = true[\"text\"].to_numpy()\n",
    "\n",
    "\n",
    "# Combine both texts to a single text\n",
    "all_texts = np.append(fake_texts, values=true_texts)\n",
    "#Create variable with 0 and 1 dependent on the length of the text arrays\n",
    "labels = np.append(np.zeros(len(fake_texts)), np.ones(len(true_texts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "Steps:\n",
    "1. Converting Texts to lowercase \n",
    "2. Stopword Removal\n",
    "3. Delete \"Reuters\"\n",
    "4. Stemming\n",
    "5. Pruning\n",
    "6. Removing Twitter's '@' and dates (e.g. \"Donald J. Trump (@realDonaldTrump) December 31, 2017Trump\")\n",
    "7. Store Preprocessing results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Text to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase = False\n",
    "\n",
    "# Iterating through all texts setting them to lowercase\n",
    "for i in range(0, all_texts.size):\n",
    "    all_texts[i] = all_texts[i].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal & Delete \"Reuters\" / \"reuters\" & Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Stopwords and Stemmer\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a token pattern\n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\") # split on whitespace\n",
    "\n",
    "def tokenize(text):\n",
    "    # Apply stopwords set from the english language. \n",
    "    my_stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "    # Add custom words to the stopwords list\n",
    "    my_stopwords.add(\"Reuters\")\n",
    "    my_stopwords.add(\"reuters\")\n",
    "\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stems = []\n",
    "    \n",
    "    # Find all items that match the previously defined token pattern in the text, that has been given as a parameter\n",
    "    tokens = token_pattern.findall(text)\n",
    "    for item in tokens:\n",
    "        if item not in my_stopwords:\n",
    "            # For every item that is not included in the stopwords list, add the stem of this word to the \"stems\" array. \n",
    "            stems.append(stemmer.stem(item))\n",
    "    return stems\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid error: rerun first cell with imports before running this cell\n",
    "\n",
    "# Vectorize the stems of all the words\n",
    "stem_vectorizer = TfidfVectorizer(tokenizer=tokenize, min_df=0.001, max_df=0.9) #critical values selected from research papers \n",
    "matrix = stem_vectorizer.fit_transform(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO Explanation\n",
    "\n",
    "features = pd.DataFrame(matrix.toarray(), columns=stem_vectorizer.get_feature_names_out())\n",
    "pd.set_option('display.max_columns', 50)\n",
    "display(features.head())\n",
    "print(features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precautionary step: Check whether the custom stopwords\"R/reuters\" have been removed from the text\n",
    "for colname in features.columns:\n",
    "    if (colname == \"reuters\"):\n",
    "        print(\"Reuters has been successfully removed\")\n",
    "    if (colname == \"Reuters\"):\n",
    "        print(\"reuters has been successfully removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store preprocessing results\n",
    "The preprocessed matrix and the label array are stored together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a matrix to a file\n",
    "import scipy.sparse\n",
    "scipy.sparse.save_npz(\"preprocessed_matrix\", matrix)\n",
    "np.save(\"preprocessed_labels\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"preprocessed_texts\", all_texts)\n",
    "#np.save(\"preprocessed_labels\", labels)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f51407d5e6c0c235947a8c224cba5ff40522160ec4fcd13db58d05d1d9585e54"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
