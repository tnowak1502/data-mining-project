{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Kaggle Data\n",
    "\n",
    "## 1. Imports \n",
    "\n",
    "Steps:\n",
    "1. Import \"pandas\" to import a .csv file from the file system.\n",
    "2. Import \"TfidfVectorizer\" to convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "3. Import \"CountVectorizer\" to convert a collection of text documents to a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Critical Imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read file\n",
    "\n",
    "Steps: \n",
    "1. Importing datasets ( https://www.kaggle.com/code/vpkprasanna/basic-text-cleaning-wordcloud-and-n-gram-analysis#Merging-true-and-fake-news-dataset )\n",
    "2. Converting datasets\n",
    "3. Combining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# importing the fake and the true dataset from the file system\n",
    "fake = pd.read_csv(\"Fake.csv\")\n",
    "true = pd.read_csv(\"True.csv\")\n",
    "\n",
    "# Convert each text of a dataset to a NumPy Array\n",
    "fake_texts = fake[\"text\"].to_numpy()\n",
    "true_texts = true[\"text\"].to_numpy()\n",
    "\n",
    "\n",
    "# Combine both texts to a single text\n",
    "all_texts = np.append(fake_texts, values=true_texts)\n",
    "#Create variable with 0 and 1 dependent on the length of the text arrays\n",
    "labels = np.append(np.zeros(len(fake_texts)), np.ones(len(true_texts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "Steps:\n",
    "1. Converting Texts to lowercase \n",
    "2. Stopword Removal\n",
    "3. Delete \"Reuters\"\n",
    "4. Stemming\n",
    "5. Pruning\n",
    "6. Removing Twitter's '@' and dates (e.g. \"Donald J. Trump (@realDonaldTrump) December 31, 2017Trump\")\n",
    "7. Store Preprocessing results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Text to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase = False\n",
    "\n",
    "# Iterating through all texts setting them to lowercase\n",
    "for i in range(0, all_texts.size):\n",
    "    all_texts[i] = all_texts[i].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal & Delete \"Reuters\" / \"reuters\" & Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yanni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Importing Stopwords and Stemmer\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a token pattern\n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\") # split on whitespace\n",
    "\n",
    "def tokenize(text):\n",
    "    # Apply stopwords set from the english language. \n",
    "    my_stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "    # Add custom words to the stopwords list\n",
    "    my_stopwords.add(\"Reuters\")\n",
    "    my_stopwords.add(\"reuters\")\n",
    "\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stems = []\n",
    "    \n",
    "    # Find all items that match the previously defined token pattern in the text, that has been given as a parameter\n",
    "    tokens = token_pattern.findall(text)\n",
    "    for item in tokens:\n",
    "        if item not in my_stopwords:\n",
    "            # For every item that is not included in the stopwords list, add the stem of this word to the \"stems\" array. \n",
    "            stems.append(stemmer.stem(item))\n",
    "    return stems\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid error: rerun first cell with imports before running this cell\n",
    "\n",
    "# Vectorize the stems of all the words\n",
    "stem_vectorizer = TfidfVectorizer(tokenizer=tokenize, min_df=0.001, max_df=0.9) #critical values selected from research papers \n",
    "matrix = stem_vectorizer.fit_transform(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>038</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>100th</th>\n",
       "      <th>101</th>\n",
       "      <th>109</th>\n",
       "      <th>10th</th>\n",
       "      <th>11</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>118</th>\n",
       "      <th>11th</th>\n",
       "      <th>12</th>\n",
       "      <th>120</th>\n",
       "      <th>122</th>\n",
       "      <th>125</th>\n",
       "      <th>...</th>\n",
       "      <th>yuan</th>\n",
       "      <th>yugoslavia</th>\n",
       "      <th>yve</th>\n",
       "      <th>zach</th>\n",
       "      <th>zakharova</th>\n",
       "      <th>zanu</th>\n",
       "      <th>zarif</th>\n",
       "      <th>zarrab</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zeid</th>\n",
       "      <th>zeitung</th>\n",
       "      <th>zero</th>\n",
       "      <th>zika</th>\n",
       "      <th>zimbabw</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zimmerman</th>\n",
       "      <th>zink</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zone</th>\n",
       "      <th>zor</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuma</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000   01  038   08   09   10  100  1000  100th  101  109  10th  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0   0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0   0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0   0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0   0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0   0.0   \n",
       "\n",
       "         11  110  111  112  115  116  118  11th   12  120  122  125  ...  \\\n",
       "0  0.028931  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       "1  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       "4  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "   yuan  yugoslavia  yve  zach  zakharova  zanu  zarif  zarrab  zealand  zeid  \\\n",
       "0   0.0         0.0  0.0   0.0        0.0   0.0    0.0     0.0      0.0   0.0   \n",
       "1   0.0         0.0  0.0   0.0        0.0   0.0    0.0     0.0      0.0   0.0   \n",
       "2   0.0         0.0  0.0   0.0        0.0   0.0    0.0     0.0      0.0   0.0   \n",
       "3   0.0         0.0  0.0   0.0        0.0   0.0    0.0     0.0      0.0   0.0   \n",
       "4   0.0         0.0  0.0   0.0        0.0   0.0    0.0     0.0      0.0   0.0   \n",
       "\n",
       "   zeitung  zero  zika  zimbabw  zimbabwean  zimmerman  zink  zionist  zip  \\\n",
       "0      0.0   0.0   0.0      0.0         0.0        0.0   0.0      0.0  0.0   \n",
       "1      0.0   0.0   0.0      0.0         0.0        0.0   0.0      0.0  0.0   \n",
       "2      0.0   0.0   0.0      0.0         0.0        0.0   0.0      0.0  0.0   \n",
       "3      0.0   0.0   0.0      0.0         0.0        0.0   0.0      0.0  0.0   \n",
       "4      0.0   0.0   0.0      0.0         0.0        0.0   0.0      0.0  0.0   \n",
       "\n",
       "   zipper  zone  zor  zuckerberg  zuma  zurich  \n",
       "0     0.0   0.0  0.0         0.0   0.0     0.0  \n",
       "1     0.0   0.0  0.0         0.0   0.0     0.0  \n",
       "2     0.0   0.0  0.0         0.0   0.0     0.0  \n",
       "3     0.0   0.0  0.0         0.0   0.0     0.0  \n",
       "4     0.0   0.0  0.0         0.0   0.0     0.0  \n",
       "\n",
       "[5 rows x 9144 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['00', '000', '01', '038', '08', '09', '10', '100', '1000', '100th',\n",
      "       ...\n",
      "       'zimmerman', 'zink', 'zionist', 'zip', 'zipper', 'zone', 'zor',\n",
      "       'zuckerberg', 'zuma', 'zurich'],\n",
      "      dtype='object', length=9144)\n"
     ]
    }
   ],
   "source": [
    "# TODO Explanation\n",
    "\n",
    "features = pd.DataFrame(matrix.toarray(), columns=stem_vectorizer.get_feature_names_out())\n",
    "pd.set_option('display.max_columns', 50)\n",
    "display(features.head())\n",
    "print(features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precautionary step: Check whether the custom stopwords\"R/reuters\" have been removed from the text\n",
    "for colname in features.columns:\n",
    "    if (colname == \"reuters\"):\n",
    "        print(\"Reuters has been successfully removed\")\n",
    "    if (colname == \"Reuters\"):\n",
    "        print(\"reuters has been successfully removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store preprocessing results\n",
    "The preprocessed matrix and the label array are stored together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a matrix to a file\n",
    "import scipy.sparse\n",
    "scipy.sparse.save_npz(\"preprocessed_matrix\", matrix)\n",
    "np.save(\"preprocessed_labels\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"preprocessed_texts\", all_texts)\n",
    "#np.save(\"preprocessed_labels\", labels)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f51407d5e6c0c235947a8c224cba5ff40522160ec4fcd13db58d05d1d9585e54"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
